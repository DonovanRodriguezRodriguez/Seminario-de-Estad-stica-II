{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c86f76d-8f0f-421b-b8cc-a09e2e1daa6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Seminario de Estadística II - Tarea 1 Parte 2\n",
    "\n",
    "**Integrantes del equipo:**\n",
    "* Azpeitia Medina Samuel\n",
    "* Castro Pérez Juan Antonio\n",
    "* Rodríguez Rodríguez Donovan Zuriel\n",
    "\n",
    "Primero vamos a configurar nuestro espacio de trabajo en Unity Catalog usando SQL. Con esto nos aseguramos de tener listos el catálogo, la base de datos y el volumen donde estarán los archivos que vamos a usar a lo largo del ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1feb280c-7cf6-4170-a868-ecce4a729300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS dev;\n",
    "CREATE DATABASE IF NOT EXISTS dev.ciencias_data;\n",
    "CREATE VOLUME IF NOT EXISTS dev.ciencias_data.session_data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c22c2624-1f8b-4ca5-964f-12fa148204e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Cree una tabla bronce en formato delta y particionado por hora para session_part1.csv.\n",
    "\n",
    "**Solución:** \n",
    "Para este primer punto vamos a leer el archivo csv original indicando que el separador es el pipe `|`. Después, crearemos una columna nueva llamada `part_hour` que extraiga la fecha y la hora del campo `timestamp` para usarla como nuestra partición. Finalmente, guardamos el dataframe como una tabla en formato delta dentro de nuestra base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7223c6e-ee32-4d24-8c21-cedf025c25ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Leemos el archivo csv de la primera parte\n",
    "df_part1 = spark.read.format(\"csv\").option(\"sep\", \"|\").option(\"header\", \"true\").load(\"/Volumes/dev/ciencias_data/session_data/sessions_part1.csv\")\n",
    "\n",
    "# Creamos la columna para particionar por hora\n",
    "df_bronce = df_part1.withColumn(\"part_hour\", F.date_format(F.to_timestamp(\"timestamp\"), \"yyyy-MM-dd-HH\"))\n",
    "\n",
    "# Guardamos la tabla particionada en formato delta\n",
    "df_bronce.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(\"part_hour\").saveAsTable(\"dev.ciencias_data.bronze_sessions\")\n",
    "\n",
    "# Visualizamos la tabla bronce\n",
    "df_bronce.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "967ba40b-8cfd-46bc-9790-5802ae40c657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. El dataset de session part1.csv corresponden a datos de tráfico de red realizados por el sniffer Arkime, cada fila es una sesión. A continuación realice lo siguiente:\n",
    "\n",
    "**Investigue el posible significado de cada campo y redacte un posible diccionario de datos.**\n",
    "\n",
    "**Solución:**\n",
    "Buscando información sobre el sniffer Arkime, armamos este diccionario de datos básico:\n",
    "* **timestamp**: Es la fecha y hora exacta en la que se capturó la sesión.\n",
    "* **srcIp / dstIp**: La dirección IP de origen (quien inició la conexión) y la de destino.\n",
    "* **srcPort / dstPort**: Los puertos que se usaron para la comunicación.\n",
    "* **srcMac / dstMac**: Arreglo con las direcciones MAC físicas por donde pasaron los datos.\n",
    "* **protocol**: Arreglo de los protocolos de red identificados (por ejemplo: tcp, udp, dns).\n",
    "* **totPackets / srcPackets / dstPackets**: El número de paquetes que se mandaron y recibieron en total.\n",
    "* **totBytes**: El tamaño total de toda la sesión en bytes.\n",
    "* **totDataBytes**: El tamaño en bytes pero solo de los datos útiles (sin contar encabezados de red).\n",
    "* **firstPacket / lastPacket**: El momento (en epoch) en el que se vio el primer y el último paquete.\n",
    "* **packetLen**: Un arreglo que dice cuánto pesó cada uno de los paquetes enviados.\n",
    "* **srcGEO / dstGEO**: El país de donde viene y a donde va la conexión, sacado por la IP.\n",
    "* **http**: Datos extra si la conexión fue a una página web, como el host o URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "164b6333-9fff-414e-9d10-0466a6920195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Estructure la información en un dataframe donde cada columna corresponda una clave del json y cada fila son los valores del dataframe, puede ignorar las siguientes claves del json cert y packetPos**\n",
    "\n",
    "**Solución:** \n",
    "Para extraer la información del JSON de forma segura y evitar que algún error de formato en el texto rompa el proceso, vamos a crear una función (UDF) apoyándonos en la librería `ast` de Python. \n",
    "\n",
    "Para cumplir con la instrucción de ignorar las claves `cert` y `packetPos`, lo que haremos será no incluirlas cuando definamos nuestro esquema (`StructType`). De esta manera, al momento en que PySpark aplane los datos, descartará automáticamente esos dos campos sin necesidad de borrarlos después."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e67915ca-daec-429d-9463-d516ce0381b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf, explode, size\n",
    "\n",
    "# Armamos el esquema sin poner cert ni packetPos\n",
    "esquema_json = StructType([\n",
    "    StructField(\"srcIp\", StringType(), True),\n",
    "    StructField(\"dstIp\", StringType(), True),\n",
    "    StructField(\"srcPort\", LongType(), True),\n",
    "    StructField(\"dstPort\", LongType(), True),\n",
    "    StructField(\"srcMac\", ArrayType(StringType()), True),\n",
    "    StructField(\"dstMac\", ArrayType(StringType()), True),\n",
    "    StructField(\"protocol\", ArrayType(StringType()), True),\n",
    "    StructField(\"totPackets\", LongType(), True),\n",
    "    StructField(\"srcPackets\", LongType(), True),\n",
    "    StructField(\"dstPackets\", LongType(), True),\n",
    "    StructField(\"totBytes\", LongType(), True),\n",
    "    StructField(\"srcBytes\", LongType(), True),\n",
    "    StructField(\"dstBytes\", LongType(), True),\n",
    "    StructField(\"totDataBytes\", StringType(), True),\n",
    "    StructField(\"firstPacket\", LongType(), True),\n",
    "    StructField(\"lastPacket\", LongType(), True),\n",
    "    StructField(\"srcGEO\", StringType(), True),\n",
    "    StructField(\"dstGEO\", StringType(), True),\n",
    "    StructField(\"packetLen\", ArrayType(LongType()), True),\n",
    "    StructField(\"http\", StructType([StructField(\"host\", ArrayType(StringType()), True)]), True)\n",
    "])\n",
    "\n",
    "esquema_salida = ArrayType(esquema_json)\n",
    "\n",
    "# Ponemos la funcion del profe\n",
    "def parseo_seguro(data_str):\n",
    "    if data_str is None:\n",
    "        return[]\n",
    "    try:\n",
    "        parsed = ast.literal_eval(data_str)\n",
    "        if isinstance(parsed, list):\n",
    "            return parsed\n",
    "        elif isinstance(parsed, dict):\n",
    "            return [parsed]\n",
    "        return[]\n",
    "    except Exception as e:\n",
    "        return[]\n",
    "\n",
    "udf_parseo = udf(parseo_seguro, esquema_salida)\n",
    "\n",
    "# Aplicamos la funcion a la tabla bronce que hicimos en el paso 1\n",
    "df_parseado = df_bronce.withColumn(\"data_parsed\", udf_parseo(col(\"data\")))\n",
    "df_parseado = df_parseado.filter(size(col(\"data_parsed\")) > 0)\n",
    "df_explotado = df_parseado.select(\"timestamp\", \"part_hour\", explode(col(\"data_parsed\")).alias(\"columna_json\"))\n",
    "\n",
    "# Aplanamos la estructura para que queden como columnas normales\n",
    "df_estructurado = df_explotado.select(\"timestamp\", \"part_hour\", \"columna_json.*\")\n",
    "\n",
    "#Visualizamos la tabla\n",
    "df_estructurado.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d320c37-04e1-4d09-8608-7f5ff9b443a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Sustituya la columna packetLen por los sumarizados: suma total, media, mínimo y máximo. Y asigne el tipo correcto a los datos por ejemplo lastPacket debe ser transformado a fecha.**\n",
    "\n",
    "**Solución:** \n",
    "Primero vamos a convertir los campos `firstPacket` y `lastPacket` a formato Timestamp. Como vienen en milisegundos, tenemos que dividirlos entre 1000. También aprovechamos para castear `totDataBytes` que a veces viene como string vacío.\n",
    "Después, usaremos funciones de arreglos de Spark para sacar las estadísticas de `packetLen` y borraremos la columna original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98024cbb-60f6-4ac9-86e7-06129a1b0618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# 1. Arreglamos los tipos de datos y fechas\n",
    "df_tipos = df_estructurado.withColumn(\"totDataBytes\", F.when(F.col(\"totDataBytes\") == \"\", None).otherwise(F.col(\"totDataBytes\")).cast(LongType()))\n",
    "df_tipos = df_tipos.withColumn(\"firstPacket\", (F.col(\"firstPacket\") / 1000).cast(TimestampType()))\n",
    "df_tipos = df_tipos.withColumn(\"lastPacket\", (F.col(\"lastPacket\") / 1000).cast(TimestampType()))\n",
    "\n",
    "# 2. Sacamos las estadisticas del arreglo packetLen\n",
    "df_tipos = df_tipos.withColumn(\"tamano_array\", F.size(F.col(\"packetLen\")))\n",
    "df_tipos = df_tipos.withColumn(\"packet_len_suma\", F.expr(\"aggregate(packetLen, 0L, (acc, x) -> acc + x)\"))\n",
    "df_tipos = df_tipos.withColumn(\"packet_len_min\", F.array_min(F.col(\"packetLen\")))\n",
    "df_tipos = df_tipos.withColumn(\"packet_len_max\", F.array_max(F.col(\"packetLen\")))\n",
    "df_tipos = df_tipos.withColumn(\"packet_len_media\", F.col(\"packet_len_suma\") / F.col(\"tamano_array\"))\n",
    "\n",
    "# Borramos la columna original como pide la instruccion\n",
    "df_transformado = df_tipos.drop(\"packetLen\", \"tamano_array\")\n",
    "\n",
    "#Visualizamos la tabla\n",
    "df_transformado.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f612aa73-b726-4eeb-b815-3684034f0be5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Cree una tabla silver con la información transformada, los nombres de los campos deben estar en snake_case y carga únicamente la información de session_part1.csv**\n",
    "\n",
    "**Solución:** \n",
    "Para no cambiar los nombres uno por uno a mano, vamos a hacer un ciclo `for` que pase por todas las columnas y use una expresión regular para convertir las mayúsculas en minúsculas separadas por un guión bajo (snake_case). Al final, guardamos todo en nuestra base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03df2956-f5c4-4bbf-bdb8-9e282b118f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Funcion sencilla en python para cambiar formato a snake_case\n",
    "def convertir_snake_case(nombre):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', nombre)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "df_silver_final = df_transformado\n",
    "\n",
    "# Recorremos todas las columnas y las renombramos\n",
    "for columna in df_silver_final.columns:\n",
    "    df_silver_final = df_silver_final.withColumnRenamed(columna, convertir_snake_case(columna))\n",
    "\n",
    "# Guardamos como tabla silver \n",
    "df_silver_final.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"dev.ciencias_data.silver_sessions\")\n",
    "\n",
    "#Visualizamos la tabla\n",
    "df_silver_final.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6b60794-d447-4b55-bb76-faa152a2f8f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Diseñe un proceso que realice una carga incremental y cargue los datos de session_part2.csv en la tabla anteriormente creada, para ello debera aplicar las mismas transformaciones y agregaciones realizadas para la parte 1.\n",
    "\n",
    "**Solución:** \n",
    "Para hacer la carga incremental, vamos a leer el segundo archivo (`sessions_part2.csv`). Primero le agregaremos la partición por hora y luego le aplicaremos exactamente la misma secuencia de transformaciones que usamos en el paso anterior (el parseo con la UDF, el casteo de fechas, los cálculos estadísticos de `packetLen` y los nombres en snake_case). \n",
    "\n",
    "La clave de este paso es que al momento de guardar el dataframe en nuestra tabla silver, usaremos el modo `append` en lugar de `overwrite`. Esto hará que los datos nuevos se sumen a los que ya teníamos de la parte 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f32a296-07cb-4486-b239-42cacc36d969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leemos el segundo archivo csv\n",
    "df_part2 = spark.read.format(\"csv\").option(\"sep\", \"|\").option(\"header\", \"true\").load(\"/Volumes/dev/ciencias_data/session_data/sessions_part2.csv\")\n",
    "\n",
    "# Agregamos la partición por hora\n",
    "df_bronce_p2 = df_part2.withColumn(\"part_hour\", F.date_format(F.to_timestamp(\"timestamp\"), \"yyyy-MM-dd-HH\"))\n",
    "\n",
    "# Parseamos el JSON con la UDF que ya teníamos registrada arriba\n",
    "df_parseado_p2 = df_bronce_p2.withColumn(\"data_parsed\", udf_parseo(F.col(\"data\")))\n",
    "df_parseado_p2 = df_parseado_p2.filter(F.size(F.col(\"data_parsed\")) > 0)\n",
    "df_explotado_p2 = df_parseado_p2.select(\"timestamp\", \"part_hour\", F.explode(F.col(\"data_parsed\")).alias(\"columna_json\"))\n",
    "\n",
    "# Aplanamos la estructura\n",
    "df_estructurado_p2 = df_explotado_p2.select(\"timestamp\", \"part_hour\", \"columna_json.*\")\n",
    "\n",
    "# Transformaciones de tipos y packetLen (igual que en la parte 1)\n",
    "df_tipos_p2 = df_estructurado_p2.withColumn(\"totDataBytes\", F.when(F.col(\"totDataBytes\") == \"\", None).otherwise(F.col(\"totDataBytes\")).cast(LongType()))\n",
    "df_tipos_p2 = df_tipos_p2.withColumn(\"firstPacket\", (F.col(\"firstPacket\") / 1000).cast(TimestampType()))\n",
    "df_tipos_p2 = df_tipos_p2.withColumn(\"lastPacket\", (F.col(\"lastPacket\") / 1000).cast(TimestampType()))\n",
    "\n",
    "df_tipos_p2 = df_tipos_p2.withColumn(\"tamano_array\", F.size(F.col(\"packetLen\")))\n",
    "df_tipos_p2 = df_tipos_p2.withColumn(\"packet_len_suma\", F.expr(\"aggregate(packetLen, 0L, (acc, x) -> acc + x)\"))\n",
    "df_tipos_p2 = df_tipos_p2.withColumn(\"packet_len_min\", F.array_min(F.col(\"packetLen\")))\n",
    "df_tipos_p2 = df_tipos_p2.withColumn(\"packet_len_max\", F.array_max(F.col(\"packetLen\")))\n",
    "df_tipos_p2 = df_tipos_p2.withColumn(\"packet_len_media\", F.col(\"packet_len_suma\") / F.col(\"tamano_array\"))\n",
    "\n",
    "df_transformado_p2 = df_tipos_p2.drop(\"packetLen\", \"tamano_array\")\n",
    "\n",
    "# Pasamos las columnas a snake_case usando nuestra funcion de python\n",
    "df_silver_p2 = df_transformado_p2\n",
    "for columna in df_silver_p2.columns:\n",
    "    df_silver_p2 = df_silver_p2.withColumnRenamed(columna, convertir_snake_case(columna))\n",
    "\n",
    "# Guardamos en la misma tabla pero con modo APPEND para la carga incremental\n",
    "df_silver_p2.write.format(\"delta\").mode(\"append\").saveAsTable(\"dev.ciencias_data.silver_sessions\")\n",
    "\n",
    "# Cargamos nuestra tabla silver completa\n",
    "df_silver = spark.sql(\"SELECT * FROM dev.ciencias_data.silver_sessions\")\n",
    "\n",
    "#Visualizamos la tabla\n",
    "df_silver.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebe4aea8-6f0b-4739-bfd5-c97e3dd6c0b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Una vez que tenga la tabla silver estructurada realice lo siguiente:\n",
    "\n",
    "**Obtenga el número de sesiones por países destino y países origen.**\n",
    "\n",
    "**Solución:** \n",
    "Usaremos las columnas `src_geo` y `dst_geo`. Solo necesitamos agrupar por ambos campos, contarlos y ordenarlos de mayor a menor para ver las rutas más comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0761bae8-665a-4adf-b62d-71bdd9d5438d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_paises = df_silver.groupBy(\"src_geo\", \"dst_geo\").count().orderBy(F.desc(\"count\"))\n",
    "df_paises.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bbf818f-30e9-4711-a8a0-f509e7399e70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Obtenga el número de sesiones por srcIP y dstIP, así también totBytes, totDataBytes y totPackets por srcIP y Protocolo.**\n",
    "\n",
    "**Solución:**\n",
    "Esta pregunta nos pide dos cosas distintas, así que haremos dos cálculos en esta celda:\n",
    "1. Primero, agrupamos por IP de origen (`src_ip`) y de destino (`dst_ip`) y contamos cuántas sesiones hay entre ellas.\n",
    "2. Segundo, para los totales por protocolo, como la columna `protocol` es un arreglo, usamos `explode` para separar cada protocolo en una fila. Luego agrupamos por la IP de origen y el protocolo individual para sumar los bytes y paquetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07909a5e-3e6c-4aac-afb8-47fd3accf1a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parte 1: Sesiones por IPs\n",
    "df_ips = df_silver.groupBy(\"src_ip\", \"dst_ip\").count().orderBy(F.desc(\"count\"))\n",
    "\n",
    "print(\"1. Número de sesiones por IP Origen y Destino:\")\n",
    "df_ips.display()\n",
    "\n",
    "# Parte 2: Totales por IP y Protocolo\n",
    "# Separamos el arreglo de protocolos\n",
    "df_protocolos = df_silver.withColumn(\"protocolo_ind\", F.explode(F.col(\"protocol\")))\n",
    "\n",
    "# Agrupamos y sumamos\n",
    "df_metricas_proto = df_protocolos.groupBy(\"src_ip\", \"protocolo_ind\").agg(F.sum(\"tot_bytes\"), F.sum(\"tot_data_bytes\"), F.sum(\"tot_packets\"))\n",
    "\n",
    "print(\"2. Totales por SrcIP y Protocolo:\")\n",
    "df_metricas_proto.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db754fb3-7f85-4484-9d56-0cc5ecb664e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Obtenga el totBytes, totDataBytes y totPackets por srcMac y dstMac.**\n",
    "\n",
    "**Solución:**\n",
    "Las columnas de MAC (`src_mac` y `dst_mac`) son arreglos porque los paquetes pasan por varios dispositivos. Spark no deja agrupar por arreglos, así que primero los convertimos a texto separándolos por comas (`concat_ws`) y luego ya podemos agrupar y sumar los totales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daeb6211-b75a-4273-a250-8b5f0628035a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convertimos los arreglos a texto para poder agrupar\n",
    "df_macs_texto = df_silver.withColumn(\"src_mac_txt\", F.concat_ws(\",\", F.col(\"src_mac\")))\n",
    "df_macs_texto = df_macs_texto.withColumn(\"dst_mac_txt\", F.concat_ws(\",\", F.col(\"dst_mac\")))\n",
    "\n",
    "df_totales_mac = df_macs_texto.groupBy(\"src_mac_txt\", \"dst_mac_txt\").agg(F.sum(\"tot_bytes\"), F.sum(\"tot_data_bytes\"), F.sum(\"tot_packets\"))\n",
    "\n",
    "df_totales_mac.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd133d6-f432-419f-975f-f21a0cd9938e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Obtenga los valores mínimo, máximo y promedio para los totBytes, totDataBytes y totPackets por srcIp, dstIp, srcMac y dstMac**\n",
    "\n",
    "**Solución:**\n",
    "Vamos a usar el dataframe anterior que ya tiene las MACs como texto (`df_macs_texto`). Agrupamos por las 4 columnas que nos piden (IPs y MACs) y calculamos el `min`, `max` y `avg` (promedio) de cada métrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c88b3c6-b3bc-4d70-9aa5-d0eef051ce52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_estadisticas = df_macs_texto.groupBy(\"src_ip\", \"dst_ip\", \"src_mac_txt\", \"dst_mac_txt\").agg(\n",
    "    F.min(\"tot_bytes\"), F.max(\"tot_bytes\"), F.avg(\"tot_bytes\"),\n",
    "    F.min(\"tot_data_bytes\"), F.max(\"tot_data_bytes\"), F.avg(\"tot_data_bytes\"),\n",
    "    F.min(\"tot_packets\"), F.max(\"tot_packets\"), F.avg(\"tot_packets\")\n",
    ")\n",
    "\n",
    "df_estadisticas.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3758cd2b-7f05-4a79-8864-f6e75b93c059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Haga un top 5 de srcIp, srcMac con mayor número de sesiones.**\n",
    "\n",
    "**Solución:**\n",
    "Haremos dos conteos rápidos. Uno agrupando solo por la IP de origen y otro por la MAC de origen (usando la versión de texto). Ordenamos de mayor a menor y limitamos a los primeros 5 resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1138eb2d-e7cb-45a0-ba18-67adff0de498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Top 5 srcIP:\")\n",
    "df_silver.groupBy(\"src_ip\").count().orderBy(F.desc(\"count\")).limit(5).display()\n",
    "\n",
    "print(\"Top 5 srcMac:\")\n",
    "df_macs_texto.groupBy(\"src_mac_txt\").count().orderBy(F.desc(\"count\")).limit(5).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a69854b9-ea3f-4b38-945b-4bdbc8b307c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Cuente el número de srcMac, dstMac involucradas de cada sesión, es decir el tamaño del array de esos campos.**\n",
    "\n",
    "**Solución:**\n",
    "Simplemente usamos la función `size` sobre las columnas originales que tienen los arreglos para saber cuántas MACs hay en cada lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b6dff80-6bce-445c-b583-edcb74cbed09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_tamano_macs = df_silver.select(\"src_ip\", \"dst_ip\", F.size(F.col(\"src_mac\")).alias(\"conteo_src_mac\"), F.size(F.col(\"dst_mac\")).alias(\"conteo_dst_mac\"))\n",
    "df_tamano_macs.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4c05aa5-e134-43ea-a98a-201aa3cc6824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**¿Cuáles son los protocolos de red más usados?**\n",
    "\n",
    "**Solución:**\n",
    "Usamos de nuevo el `explode` en la columna `protocol` para contar cada uno por separado, ordenamos por cantidad y mostramos los más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "316b4a12-4c9c-4950-bfdd-0a62bdbb2cec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_protocolos_top = df_silver.select(F.explode(F.col(\"protocol\")).alias(\"protocolo\")).groupBy(\"protocolo\").count().orderBy(F.desc(\"count\"))\n",
    "df_protocolos_top.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc2c90d0-6537-4350-81d7-ff94a575537d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**¿Cuáles son las páginas web más visitadas?**\n",
    "\n",
    "**Solución:**\n",
    "El host está dentro de la estructura `http`. Primero filtramos para quedarnos solo con los registros que no sean nulos (que sí sean tráfico web) y luego explotamos el arreglo `http.host` para contar las páginas individuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "580ba544-92ab-49d8-9b0f-69a5bf74aeb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtramos nulos y explotamos\n",
    "df_webs = df_silver.filter(F.col(\"http.host\").isNotNull()).select(F.explode(F.col(\"http.host\")).alias(\"pagina_web\"))\n",
    "\n",
    "df_webs_top = df_webs.groupBy(\"pagina_web\").count().orderBy(F.desc(\"count\"))\n",
    "df_webs_top.display()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7362020889406707,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Tarea 1 Parte 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
